<?xml version="1.0" encoding="UTF-8"?>

<!-- This file is part of the book                -->
<!--                                              -->
<!--      A First Course in Linear Algebra        -->
<!--                                              -->
<!-- Copyright (C) 2004-2017  Robert A. Beezer    -->
<!-- See the file COPYING for copying conditions. -->

<chapter xml:id="chapter-M" acro="M">
    <title>Matrices</title>
    <introduction>
        <p>We have made frequent use of matrices for solving systems of equations, and we have begun to investigate a few of their properties, such as the null space and nonsingularity.  In this chapter, we will take a more systematic approach to the study of matrices.</p>
    </introduction>
    <xi:include xmlns:xi="http://www.w3.org/2001/XInclude" href="./section-MO.xml"/>
    <xi:include xmlns:xi="http://www.w3.org/2001/XInclude" href="./section-MM.xml"/>
    <xi:include xmlns:xi="http://www.w3.org/2001/XInclude" href="./section-MISLE.xml"/>
    <xi:include xmlns:xi="http://www.w3.org/2001/XInclude" href="./section-MINM.xml"/>
    <xi:include xmlns:xi="http://www.w3.org/2001/XInclude" href="./section-CRS.xml"/>
    <xi:include xmlns:xi="http://www.w3.org/2001/XInclude" href="./section-FS.xml"/>
    <conclusion xml:id="annoacro-M">
        <title>Annotated Acronyms M</title>
        <paragraphs><title><xref ref="theorem-VSPM" acro="VSPM"/></title>
        <p>These are the fundamental rules for working with the addition, and scalar multiplication, of matrices.  We saw something very similar in the previous chapter (<xref ref="theorem-VSPCV" acro="VSPCV"/>).  Together, these two definitions will provide our definition for the key definition, <xref ref="definition-VS" acro="VS"/>.</p>
        </paragraphs>
        <paragraphs>
            <title><xref ref="theorem-SLEMM" acro="SLEMM"/></title>
            <p><xref ref="theorem-SLSLC" acro="SLSLC"/> connected linear combinations with systems of equations.  <xref ref="theorem-SLEMM" acro="SLEMM"/> connects the matrix-vector product (<xref ref="definition-MVP" acro="MVP"/>) and column vector equality (<xref ref="definition-CVE" acro="CVE"/>) with systems of equations.   We will see this one regularly.</p>
        </paragraphs>
        <paragraphs>
            <title><xref ref="theorem-EMP" acro="EMP"/></title>
            <p>This theorem is a workhorse in <xref ref="section-MM" acro="MM"/> and will continue to make regular appearances.  If you want to get better at formulating proofs, the application of this theorem can be a key step in gaining that broader understanding.  While it might be hard to imagine <xref ref="theorem-EMP" acro="EMP"/> as a <em>definition</em> of matrix multiplication, we will see in <xref ref="exercise-MR-T80" acro="MR.T80"/> that in theory it is actually a <em>better</em> definition of matrix multiplication long-term.</p>
        </paragraphs>
        <paragraphs>
            <title><xref ref="theorem-CINM" acro="CINM"/></title>
            <p>The inverse of a matrix is key.  Here is how you can get one if you know how to row-reduce.</p>
        </paragraphs>
        <paragraphs>
            <title><xref ref="theorem-NPNT" acro="NPNT"/></title>
            <p>This theorem is a fundamental tool for proving subsequent important theorems, such as <xref ref="theorem-NI" acro="NI"/>.  It may also be the best explantion for the term <q>nonsingular.</q></p>
        </paragraphs>
        <paragraphs>
            <title><xref ref="theorem-NI" acro="NI"/></title>
            <p><q>Nonsingularity</q> or <q>invertibility</q>?  Pick your favorite, or show your versatility by using one or the other in the right context.  They mean the same thing.</p>
        </paragraphs>
        <paragraphs>
            <title><xref ref="theorem-CSCS" acro="CSCS"/></title>
            <p>Given a coefficient matrix, which vectors of constants create consistent systems?  This theorem tells us that the answer is exactly those column vectors in the column space.  Conversely, we also use this theorem to test for membership in the column space by checking the consistency of the appropriate system of equations.</p>
        </paragraphs>
        <paragraphs>
            <title><xref ref="theorem-BCS" acro="BCS"/></title>
            <p>Another theorem that provides a linearly independent set of vectors whose span equals some set of interest (a column space this time).</p>
        </paragraphs>
        <paragraphs>
            <title><xref ref="theorem-BRS" acro="BRS"/></title>
            <p>Yet another theorem that provides a linearly independent set of vectors whose span equals some set of interest (a row space).</p>
        </paragraphs>
        <paragraphs>
            <title><xref ref="theorem-CSRST" acro="CSRST"/></title>
            <p>Column spaces, row spaces, transposes, rows, columns.  Many of the connections between these objects are based on the simple observation captured in this theorem.  This is not a deep result.  We state it as a theorem for convenience, so we can refer to it as needed.</p>
        </paragraphs>
        <paragraphs>
            <title><xref ref="theorem-FS" acro="FS"/></title>
            <p>This theorem is inherently interesting, if not computationally satisfying.  Null space, row space, column space, left null space <mdash/> here they all are, simply by row reducing the extended matrix and applying <xref ref="theorem-BNS" acro="BNS"/> and <xref ref="theorem-BCS" acro="BCS"/> twice (each).  Nice.</p>
        </paragraphs>
    </conclusion>
</chapter>
