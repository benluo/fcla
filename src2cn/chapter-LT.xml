<?xml version="1.0" encoding="UTF-8"?>

<!-- This file is part of the book                -->
<!--                                              -->
<!--      A First Course in Linear Algebra        -->
<!--                                              -->
<!-- Copyright (C) 2004-2017  Robert A. Beezer    -->
<!-- See the file COPYING for copying conditions. -->

<chapter xml:id="chapter-LT" acro="LT">
    <title>Linear Transformations</title>
    <introduction>
        <p>In the next linear algebra course you take, the first lecture might be a reminder about what a vector space is (<xref ref="definition-VS" acro="VS"/>), their ten properties, basic theorems and then some examples.  The second lecture would likely be all about linear transformations.  While it may seem we have waited a long time to present what must be a central topic, in truth we have already been working with linear transformations for some time.</p>
        <p>Functions are important objects in the study of calculus, but have been absent from this course until now (well, not really, it just seems that way).  In your study of more advanced mathematics it is nearly impossible to escape the use of functions <mdash/> they are as fundamental as sets are.</p>
    </introduction>
    <xi:include xmlns:xi="http://www.w3.org/2001/XInclude" href="./section-LT.xml"/>
    <xi:include xmlns:xi="http://www.w3.org/2001/XInclude" href="./section-ILT.xml"/>
    <xi:include xmlns:xi="http://www.w3.org/2001/XInclude" href="./section-SLT.xml"/>
    <xi:include xmlns:xi="http://www.w3.org/2001/XInclude" href="./section-IVLT.xml"/>
    <conclusion xml:id="annoacro-LT"><title>Annotated Acronyms LT</title>
        <paragraphs>
            <title><xref ref="theorem-MBLT" acro="MBLT"/></title>
            <p>You give me an <m>m\times n</m> matrix and I will give you a linear transformation <m>\ltdefn{T}{\complex{n}}{\complex{m}}</m>.  This is our first hint that there is some relationship between linear transformations and matrices.</p>
        </paragraphs>
        <paragraphs>
            <title><xref ref="theorem-MLTCV" acro="MLTCV"/></title>
            <p>You give me a linear transformation <m>\ltdefn{T}{\complex{n}}{\complex{m}}</m> and I will give you an <m>m\times n</m> matrix.  This is our second hint that there is some relationship between linear transformations and matrices.  Generalizing this relationship to arbitrary vector spaces (<ie/> not just <m>\complex{n}</m> and  <m>\complex{m}</m>) will be the most important idea of <xref ref="chapter-R" acro="R"/>.</p>
        </paragraphs>
        <paragraphs>
            <title><xref ref="theorem-LTLC" acro="LTLC"/></title>
            <p>A simple idea, and as described in <xref ref="exercise-LT-T20" acro="LT.T20"/>, equivalent to the <xref ref="definition-LT" acro="LT"/>.  The statement is really just for convenience, as we will quote this one often.</p>
        </paragraphs>
        <paragraphs>
            <title><xref ref="theorem-LTDB" acro="LTDB"/></title>
            <p>Another simple idea, but a powerful one.  <q>It is enough to know what a linear transformation does to a basis.</q>  At the outset of <xref ref="chapter-R" acro="R"/>, <xref ref="theorem-VRRB" acro="VRRB"/> will help us define a very important function, and then <xref ref="theorem-LTDB" acro="LTDB"/> will allow us to understand that this function is also a linear transformation.</p>
        </paragraphs>
        <paragraphs>
            <title><xref ref="theorem-KPI" acro="KPI"/></title>
            <p>The pre-image will be an important construction in this chapter, and this is one of the most important descriptions of the pre-image.  It should remind you very much of <xref ref="theorem-PSPHS" acro="PSPHS"/>.  Also see <xref ref="theorem-RPI" acro="RPI"/>, which has a description below.</p>
        </paragraphs>
        <paragraphs>
            <title><xref ref="theorem-KILT" acro="KILT"/></title>
            <p>Kernels and injective linear transformations are intimately related.  This result is the connection.  Compare with <xref ref="theorem-RSLT" acro="RSLT"/> below.</p>
        </paragraphs>
        <paragraphs>
            <title><xref ref="theorem-ILTB" acro="ILTB"/></title>
            <p>Injective linear transformations and linear independence are intimately related.  This result is the connection.  Compare with <xref ref="theorem-SLTB" acro="SLTB"/> below.</p>
        </paragraphs>
        <paragraphs>
            <title><xref ref="theorem-RSLT" acro="RSLT"/></title>
            <p>Ranges and surjective linear transformations are intimately related.  This result is the connection.  Compare with <xref ref="theorem-KILT" acro="KILT"/> above.</p>
        </paragraphs>
        <paragraphs>
            <title><xref ref="theorem-SSRLT" acro="SSRLT"/></title>
            <p>This theorem provides the most direct way of forming the range of a linear transformation.  The resulting spanning set might well be linearly dependent, and beg for some clean-up, but that does not stop us from having very quickly formed a reasonable description of the range.  If you find the determination of spanning sets or ranges difficult, this is one worth remembering.  You can view this as the analogue of forming a column space by a direct application of <xref ref="definition-CSM" acro="CSM"/>.</p>
        </paragraphs>
        <paragraphs>
            <title><xref ref="theorem-SLTB" acro="SLTB"/></title>
            <p>Surjective linear transformations and spanning sets are intimately related.  This result is the connection.  Compare with <xref ref="theorem-ILTB" acro="ILTB"/> above.</p>
        </paragraphs>
        <paragraphs>
            <title><xref ref="theorem-RPI" acro="RPI"/></title>
            <p>This is the analogue of <xref ref="theorem-KPI" acro="KPI"/>.  Membership in the range is equivalent to nonempty pre-images.</p>
        </paragraphs>
        <paragraphs>
            <title><xref ref="theorem-ILTIS" acro="ILTIS"/></title>
            <p>Injectivity and surjectivity are independent concepts.  You can have one without the other.  But when you have both, you get invertibility, a linear transformation that can be run <q>backwards.</q>  This result might explain the entire structure of the four sections in this chapter.</p>
        </paragraphs>
        <paragraphs>
            <title><xref ref="theorem-RPNDD" acro="RPNDD"/></title>
            <p>This is the promised generalization of <xref ref="theorem-RPNC" acro="RPNC"/> about matrices.  So the number of columns of a matrix is the analogue of the dimension of the domain.  This will become even more precise in <xref ref="chapter-R" acro="R"/>.  For now, this can be a powerful result for determining dimensions of kernels and ranges, and consequently, the injectivity or surjectivity of linear transformations.  Never underestimate a theorem that counts something.</p>
        </paragraphs>
    </conclusion>
</chapter>
