<?xml version="1.0" encoding="UTF-8"?>

<!-- This file is part of the book                -->
<!--                                              -->
<!--      A First Course in Linear Algebra        -->
<!--                                              -->
<!-- Copyright (C) 2004-2017  Robert A. Beezer    -->
<!-- See the file COPYING for copying conditions. -->

<chapter xml:id="chapter-R" acro="R">
    <title>Representations</title>
    <introduction>
        <p>Previous work with linear transformations may have convinced you that we can convert most questions about linear transformations into questions about systems of equations or properties of subspaces of <m>\complex{m}</m>.  In this section we begin to make these vague notions precise.   We have used the word <q>representation</q> prior, but it will get a heavy workout in this chapter.  In many ways, everything we have studied so far was in preparation for this chapter.</p>
    </introduction>
    <xi:include xmlns:xi="http://www.w3.org/2001/XInclude" href="./section-VR.xml"/>
    <xi:include xmlns:xi="http://www.w3.org/2001/XInclude" href="./section-MR.xml"/>
    <xi:include xmlns:xi="http://www.w3.org/2001/XInclude" href="./section-CB.xml"/>
    <xi:include xmlns:xi="http://www.w3.org/2001/XInclude" href="./section-OD.xml"/>
    <conclusion xml:id="annoacro-R"><title>Annotated Acronyms R</title>
        <paragraphs>
            <title><xref ref="definition-VR" acro="VR"/></title>
            <p>Matrix representations build on vector representations, so this is the definition that gets us started.  A representation depends on the choice of a single basis for the vector space.  <xref ref="theorem-VRRB" acro="VRRB"/> is what tells us this idea might be useful.</p>
        </paragraphs>
        <paragraphs>
            <title><xref ref="theorem-VRILT" acro="VRILT"/></title>
            <p>As an invertible linear transformation, vector representation allows us to translate, back and forth, between abstract vector spaces (<m>V</m>) and concrete vector spaces (<m>\complex{n}</m>).  This is key to all our notions of representations in this chapter.</p>
        </paragraphs>
        <paragraphs>
            <title><xref ref="theorem-CFDVS" acro="CFDVS"/></title>
            <p>Every vector space with finite dimension <q>looks like</q> a vector space of column vectors.  Vector representation is the isomorphism that establishes that these vector spaces are isomorphic.</p>
        </paragraphs>
        <paragraphs>
            <title><xref ref="definition-MR" acro="MR"/></title>
            <p>Building on the definition of a vector representation, we define a representation of a linear transformation, determined by a choice of two bases, one for the domain and one for the codomain.  Notice that vectors are represented by columnar lists of scalars, while linear transformations are represented by rectangular tables of scalars.  Building a matrix representation is as important a skill as row-reducing a matrix.</p>
        </paragraphs>
        <paragraphs>
            <title><xref ref="theorem-FTMR" acro="FTMR"/></title>
            <p><xref ref="definition-MR" acro="MR"/> is not really very interesting until we have this theorem.  The second form tells us that we can compute outputs of linear transformations via matrix multiplication, along with some bookkeeping for vector representations.  Searching forward through the text on <q>FTMR</q> is an interesting exercise.  You will find reference to this result buried inside many key proofs at critical points, and it also appears in numerous examples and solutions to exercises.</p>
        </paragraphs>
        <paragraphs>
            <title><xref ref="theorem-MRCLT" acro="MRCLT"/></title>
            <p>Turns out that matrix multiplication is really a very natural operation, it is just the chaining together (composition) of functions (linear transformations).  Beautiful.  Even if you do not try to work the problem, study <xref ref="solution-MR-T80" acro="MR.T80"/> for more insight.</p>
        </paragraphs>
        <paragraphs>
            <title><xref ref="theorem-KNSI" acro="KNSI"/></title>
            <p>Kernels <q>are</q> null spaces.  For this reason you will see these terms used interchangeably.</p>
        </paragraphs>
        <paragraphs>
            <title><xref ref="theorem-RCSI" acro="RCSI"/></title>
            <p>Ranges <q>are</q> column spaces.  For this reason you will see these terms used interchangeably.</p>
        </paragraphs>
        <paragraphs>
            <title><xref ref="theorem-IMR" acro="IMR"/></title>
            <p>Invertible linear transformations are represented by invertible (nonsingular) matrices.</p>
        </paragraphs>
        <paragraphs>
            <title><xref ref="theorem-NME9" acro="NME9"/></title>
            <p>The NMEx series has always been important, but we have held off saying so until now.  This is the end of the line for this one, so it is a good time to contemplate all that it means.</p>
        </paragraphs>
        <paragraphs>
            <title><xref ref="theorem-SCB" acro="SCB"/></title>
            <p>Diagonalization back in <xref ref="section-SD" acro="SD"/> was really a change of basis to achieve a diagonal matrix repesentation.  Maybe we should be highlighting the more general <xref ref="theorem-MRCB" acro="MRCB"/> here, but its overly technical description just is not as appealing.  However, it will be important in some of the matrix decompositions you will encounter in a future course in linear algebra.</p>
        </paragraphs>
        <paragraphs>
            <title><xref ref="theorem-EER" acro="EER"/></title>
            <p>This theorem, with the companion definition, <xref ref="definition-EELT" acro="EELT"/>, tells us that eigenvalues, and eigenvectors, are fundamentally a characteristic of linear transformations (not matrices).  If you study matrix decompositions in a future course in linear algebra you will come to appreciate that almost all of a matrix's secrets can be unlocked with knowledge of the eigenvalues and eigenvectors.</p>
        </paragraphs>
        <paragraphs>
            <title><xref ref="theorem-OD" acro="OD"/></title>
            <p>Can you imagine anything nicer than an orthonormal diagonalization?  A basis of pairwise orthogonal, unit norm, eigenvectors that provide a diagonal representation for a matrix?  Here we learn just when this can happen <mdash/> precisely when a matrix is normal, which is a disarmingly simple property to define.</p>
        </paragraphs>
    </conclusion>
</chapter>
