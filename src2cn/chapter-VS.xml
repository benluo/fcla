<?xml version="1.0" encoding="UTF-8"?>

<!-- This file is part of the book                -->
<!--                                              -->
<!--      A First Course in Linear Algebra        -->
<!--                                              -->
<!-- Copyright (C) 2004-2017  Robert A. Beezer    -->
<!-- See the file COPYING for copying conditions. -->

<chapter xml:id="chapter-VS" acro="VS">
    <title>Vector Spaces</title>
    <introduction>
        <p>We now have a computational toolkit in place and so we can begin our study of linear algebra at a more theoretical level.</p>
        <p>Linear algebra is the study of two fundamental objects, vector spaces and linear transformations (see <xref ref="chapter-LT" acro="LT"/>).  This chapter will focus on the former.  The power of mathematics is often derived from generalizing many different situations into one abstract formulation, and that is exactly what we will be doing throughout this chapter.</p>
    </introduction>
    <xi:include xmlns:xi="http://www.w3.org/2001/XInclude" href="./section-VS.xml"/>
    <xi:include xmlns:xi="http://www.w3.org/2001/XInclude" href="./section-S.xml"/>
    <xi:include xmlns:xi="http://www.w3.org/2001/XInclude" href="./section-LISS.xml"/>
    <xi:include xmlns:xi="http://www.w3.org/2001/XInclude" href="./section-B.xml"/>
    <xi:include xmlns:xi="http://www.w3.org/2001/XInclude" href="./section-D.xml"/>
    <xi:include xmlns:xi="http://www.w3.org/2001/XInclude" href="./section-PD.xml"/>
    <conclusion xml:id="annoacro-VS">
        <title>Annotated Acronyms VS</title>
        <paragraphs>
            <title><xref ref="definition-VS" acro="VS"/></title>
            <p>The most fundamental object in linear algebra is a vector space.  Or else the most fundamental object is a vector, and a vector space is important because it is a collection of vectors.  Either way, <xref ref="definition-VS" acro="VS"/> is critical.  All of our remaining theorems that assume we are working with a vector space can trace their lineage back to this definition.</p>
        </paragraphs>
        <paragraphs>
            <title><xref ref="theorem-TSS" acro="TSS"/></title>
            <p>Checking all ten properties of a vector space (<xref ref="definition-VS" acro="VS"/>) can get tedious.  But if you have a subset of a <em>known</em> vector space, then <xref ref="theorem-TSS" acro="TSS"/> considerably shortens the verification.  Also, proofs of closure (the last two conditions in <xref ref="theorem-TSS" acro="TSS"/>) are a good way to practice a common style of proof.</p>
        </paragraphs>
        <paragraphs>
            <title><xref ref="theorem-VRRB" acro="VRRB"/></title>
            <p>The proof of uniqueness in this theorem is a very typical employment of the hypothesis of linear independence.  But that is not why we mention it here.  This theorem is critical to our first section about representations, <xref ref="section-VR" acro="VR"/>, via <xref ref="definition-VR" acro="VR"/>.</p>
        </paragraphs>
        <paragraphs>
            <title><xref ref="theorem-CNMB" acro="CNMB"/></title>
            <p>Having just defined a basis (<xref ref="definition-B" acro="B"/>) we discover that the columns of a nonsingular matrix form a basis of <m>\complex{m}</m>.  Much of what we know about nonsingular matrices is either contained in this statement, or much more evident because of it.</p>
        </paragraphs>
        <paragraphs>
            <title><xref ref="theorem-SSLD" acro="SSLD"/></title>
            <p>This theorem is a key juncture in our development of linear algebra.  You have probably already realized how useful <xref ref="theorem-G" acro="G"/> is.  All four parts of <xref ref="theorem-G" acro="G"/> have proofs that finish with an application of <xref ref="theorem-SSLD" acro="SSLD"/>.</p>
        </paragraphs>
        <paragraphs>
            <title><xref ref="theorem-RPNC" acro="RPNC"/></title>
            <p>This simple relationship between the rank, nullity and number of columns of a matrix might be surprising.  But in simplicity comes power, as this theorem can be very useful.  It will be generalized in the very last theorem of <xref ref="chapter-LT" acro="LT"/>, <xref ref="theorem-RPNDD" acro="RPNDD"/>.</p>
        </paragraphs>
        <paragraphs>
            <title><xref ref="theorem-G" acro="G"/></title>
            <p>A whimsical title, but the intent is to make sure you do not miss this one.  Much of the interaction between bases, dimension, linear independence, and spanning is captured in this theorem.</p>
        </paragraphs>
        <paragraphs>
            <title><xref ref="theorem-RMRT" acro="RMRT"/></title>
            <p>This one is a real surprise.  Why should a matrix, and its transpose, both row-reduce to the same number of nonzero rows?</p>
        </paragraphs>
    </conclusion>
</chapter>
