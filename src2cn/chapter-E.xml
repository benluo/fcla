<?xml version="1.0" encoding="UTF-8"?>

<!-- This file is part of the book                -->
<!--                                              -->
<!--      A First Course in Linear Algebra        -->
<!--                                              -->
<!-- Copyright (C) 2004-2017  Robert A. Beezer    -->
<!-- See the file COPYING for copying conditions. -->

<chapter xml:id="chapter-E" acro="E">
    <title>Eigenvalues</title>
    <introduction>
        <p>When we have a square matrix of size <m>n</m>, <m>A</m>, and we multiply it by a vector <m>\vect{x}</m> from <m>\complex{n}</m> to form the matrix-vector product (<xref ref="definition-MVP" acro="MVP"/>), the result is another vector in <m>\complex{n}</m>.  So we can adopt a functional view of this computation <mdash/> the act of multiplying by a square matrix is a function that converts one vector (<m>\vect{x}</m>) into another one (<m>A\vect{x}</m>) of the same size.  For some vectors, this seemingly complicated computation is really no more complicated than scalar multiplication.  The vectors vary according to the choice of <m>A</m>, so the question is to determine, for an individual choice of <m>A</m>, if there are any such vectors, and if so, which ones.  It happens in a variety of situations that these vectors (and the scalars that go along with them) are of special interest.</p>
        <p>We will be solving polynomial equations in this chapter, which raises the specter of complex numbers as roots.  This distinct possibility is our main reason for entertaining the complex numbers throughout the course.  You might be moved to revisit <xref ref="section-CNO" acro="CNO"/> and <xref ref="section-O" acro="O"/>.</p>
    </introduction>
    <xi:include xmlns:xi="http://www.w3.org/2001/XInclude" href="./section-EE.xml"/>
    <xi:include xmlns:xi="http://www.w3.org/2001/XInclude" href="./section-PEE.xml"/>
    <xi:include xmlns:xi="http://www.w3.org/2001/XInclude" href="./section-SD.xml"/>
    <conclusion xml:id="annoacro-E">
        <title>Annotated Acronyms E</title>
        <paragraphs>
            <title><xref ref="theorem-EMRCP" acro="EMRCP"/></title>
            <p>Much of what we know about eigenvalues can be traced to analysis of the characteristic polynomial.  When we first defined eigenvalues, you might have wondered if they were scarce, or abundant.  The characteristic polynomial allows us to answer a question like this with a result like <xref ref="theorem-NEM" acro="NEM"/> which tells us there are always a few eigenvalues, but never too many.</p>
        </paragraphs>
        <paragraphs>
            <title><xref ref="theorem-EMNS" acro="EMNS"/></title>
            <p>If <xref ref="theorem-EMRCP" acro="EMRCP"/> allows us to learn about eigenvalues through what we know about roots of polynomials, then <xref ref="theorem-EMNS" acro="EMNS"/> allows us to learn about eigenvectors, and eigenspaces, from what we already know about null spaces.  These two theorems, along with <xref ref="definition-EEM" acro="EEM"/>, provide the starting points for discerning the properties of eigenvalues and eigenvectors (to say nothing of actually computing them).</p>
        </paragraphs>
        <paragraphs>
            <title><xref ref="theorem-HMRE" acro="HMRE"/></title>
            <p>As we have remarked before, we choose to include all of the complex numbers in our set of allowed scalars, whereas many introductory texts restrict their attention to just the real numbers.  Here is one of the payoffs to this approach.  Begin with a matrix, possibly containing complex entries, and require the matrix to be Hermitian (<xref ref="definition-HM" acro="HM"/>).  In the case of only real entries, this boils down to just requiring the matrix to be symmetric (<xref ref="definition-SYM" acro="SYM"/>).  Generally, the roots of a characteristic polynomial, even with all real coefficients, can have complex numbers as roots.  But for a Hermitian matrix, all of the eigenvalues are real numbers!  When somebody tells you mathematics can be beautiful, this is an example of what they are talking about.</p>
        </paragraphs>
        <paragraphs>
            <title><xref ref="theorem-DC" acro="DC"/></title>
            <p>Diagonalizing a matrix, or the question of if a matrix is diagonalizable, could be viewed as one of a handful of central questions in linear algebra.  Here we have an unequivocal answer to the question of <q>if,</q> along with a proof containing a construction for the diagonalization.  So this theorem is of theoretical and computational interest.  This topic will be important again in <xref ref="chapter-R" acro="R"/>.</p>
        </paragraphs>
        <paragraphs>
            <title><xref ref="theorem-DMFE" acro="DMFE"/></title>
            <p>Another unequivocal answer to the question of if a matrix is diagonalizable, with perhaps a simpler condition to test.  The proof also tells us how to construct the necessary set of <m>n</m> linearly independent eigenvectors <mdash/> just round up bases for each eigenspace and join them together.  No need to test the linear independence of the combined set.</p>
        </paragraphs>
    </conclusion>
</chapter>
